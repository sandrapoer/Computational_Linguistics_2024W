{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/cl_intro_ws2024/blob/main/tutorials/Tutorial6_model_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 6: Introduction to Computational Linguistics\n",
        "\n",
        "This is the fifth tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2024. Hands-on exercises are marked with ğŸ‘‹ âš’ and questions are marked with â“. Remember to first **store this notebook** in your Drive or GitHub.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dYdgeFnF9dhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **Lesson 1: Using Large Language Models in Colab**\n",
        "\n",
        "There are many ways to use Large Language Models (LLMs) in Colab and other environments. We will use the environment [Ollama](https://ollama.com/) today. Another option that provides access to a range of LLMs is [Unsloth](https://docs.unsloth.ai/).\n",
        "\n",
        "We will first install [colab-xterm](https://github.com/InfuseAI/colab-xterm) that allows us to run a terminal within a code cell on Google Colab.\n"
      ],
      "metadata": {
        "id": "n3GcvJ9I9m8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B99T0uX5N5Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47209ba-1836-415a-ba6f-0d186afeb4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-xterm\n",
            "  Downloading colab_xterm-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (6.3.3)\n",
            "Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: colab-xterm\n",
            "Successfully installed colab-xterm-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type and run these two lines one after another in below terminal after run the cell (%xterm) to install and run Ollama:\n",
        "* curl -fsSL https://ollama.com/install.sh | sh\n",
        "* ollama serve & ollama pull llama3.2"
      ],
      "metadata": {
        "id": "w2RKunFoBiwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Pw_eekOpuW"
      },
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then use [LangChain](https://python.langchain.com/docs/introduction/), a library to support the use of LLMs."
      ],
      "metadata": {
        "id": "1fkxo1E_C_VG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovlpFAQbPpYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9319238d-ea21-4f41-97dd-b00ffd27f709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.4/2.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install langchain\n",
        "!pip -qq install langchain-core\n",
        "!pip -qq install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7gKz7iUPJ6y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "e5a8e16a-3d80-4d00-afb0-e8ece1733636"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The meaning of life is a question that has puzzled philosophers, theologians, scientists, and everyday people for centuries. There is no one definitive answer, as it often depends on cultural, personal, and philosophical perspectives. Here are some possible interpretations:\\n\\n1. **Hedonic Pursuit**: The pursuit of happiness and pleasure. According to Epicureanism, the meaning of life is to seek happiness and avoid pain.\\n2. **Self-Actualization**: The realization of one's full potential and becoming the best version of oneself. This concept was introduced by Abraham Maslow in his theory of human motivation.\\n3. **Spiritual Growth**: Finding purpose and fulfillment through spiritual practices, such as meditation, prayer, or devotion to a higher power.\\n4. **Social Connection**: Building and maintaining relationships with others, which provides a sense of belonging, love, and support.\\n5. **Personal Legacy**: Leaving a lasting impact on the world, creating something that will outlast oneself, and being remembered by future generations.\\n6. **Meaning through Work**: Finding meaning in one's profession or occupation, where work is an extension of one's values and passions.\\n7. **Existentialism**: Embracing the present moment and finding purpose through individual freedom and choice.\\n8. **Scientific Explanation**: The universe has a predetermined plan, and human life has a specific purpose that can be discovered through science and empirical evidence.\\n9. **Cosmological Perspective**: The meaning of life is to contribute to the greater good of the universe, following the principles of cosmic evolution and the interconnectedness of all things.\\n10. **Transcendence**: Connecting with something greater than oneself, such as a higher power, nature, or a sense of oneness with the universe.\\n\\nUltimately, the meaning of life is a deeply personal question that each individual must explore and answer for themselves. What brings purpose and fulfillment to one person's life may not be the same for another.\\n\\nSome philosophical theories that attempt to explain the meaning of life include:\\n\\n* **Stoicism**: Emphasis on reason, self-control, and indifference to external events.\\n* **Utilitarianism**: The pursuit of happiness and well-being for the greatest number of people.\\n* **Nihilism**: The rejection of traditional values and meaning in life.\\n* **Absurdism**: The acceptance of uncertainty and the search for meaning in a seemingly meaningless world.\\n\\nRemember, these are just a few examples, and there is no one-size-fits-all answer to this profound question.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from langchain.llms import Ollama\n",
        "llm = Ollama(model = \"llama3.2\")\n",
        "llm.predict(\"what is the Meaning of life\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"The typical color of the sky is: \")"
      ],
      "metadata": {
        "id": "i7qnuV2of9kp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e16cef37-6d34-4fe4-c2d7-4bb109a0e424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Blue.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"which model version are you?\")"
      ],
      "metadata": {
        "id": "J8dfzFiukxob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "849000e0-a862-44c4-ce6a-4284219e02a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I\\'m a large language model, so I don\\'t have a specific \"model version\" in the classical sense. My architecture and training data are based on a combination of models and techniques that are constantly evolving.\\n\\nThat being said, my knowledge was last updated in December 2023, and I use a model architecture based on transformer technology, which is a popular approach for natural language processing tasks.\\n\\nIf you\\'re looking for more specific information about my capabilities or limitations, I can try to provide more details. Or, if you have any questions or topics you\\'d like to discuss, I\\'m here to help!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Describe quantum physics in one short sentence of no more than 12 words\")"
      ],
      "metadata": {
        "id": "j_W7v2EblBWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ca87417b-b2fd-4e1b-9c30-031e705d7b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Quantum physics describes the behavior of tiny particles' random, wave-like interactions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Explain the latest advances in large language models to me.\")"
      ],
      "metadata": {
        "id": "SUQ2cVkElNLj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "31325633-3572-454f-c936-61f398ee3b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The field of large language models (LLMs) has experienced significant advancements in recent years, leading to improved performance, efficiency, and versatility. Here are some of the latest developments:\\n\\n1. **Multitask Learning**: Many modern LLMs are designed to learn multiple tasks simultaneously, such as text classification, sentiment analysis, and question answering. This approach enables models to adapt to diverse applications and improve overall performance.\\n2. **Efficient Attention Mechanisms**: Researchers have developed more efficient attention mechanisms, such as token-level attention, position-aware attention, and spatial attention. These advancements enable faster inference times while maintaining or improving model accuracy.\\n3. **Transformer-XL and BERT-MLM**: The Transformer-XL architecture introduced a novel scaling strategy for Transformers, allowing models to be trained on larger datasets without significant performance degradation. BERT-MLM (Bidirectional Encoder Representations from Transformers with Masked Language Modeling) further improved upon this by incorporating self-supervised learning techniques.\\n4. **Multimodal Models**: The integration of multimodal inputs (e.g., images, audio, or text) into LLMs has become increasingly important. Multimodal models can leverage visual and auditory cues to enhance understanding and improve performance in tasks like image captioning, visual question answering, and conversational AI.\\n5. **Adversarial Training**: Adversarial training involves exposing models to intentionally crafted input sequences designed to mislead or deceive the model. This technique helps improve robustness against adversarial attacks, ensuring that LLMs can maintain their accuracy in the face of malicious inputs.\\n6. **Memory-Augmented Models**: Memory-augmented models incorporate external memory mechanisms into traditional Transformers. These augmentations enable models to retain information from previous interactions and adapt to changing contexts, leading to improved performance in tasks like conversational AI and question answering.\\n7. **Conversational AI Advances**: LLMs have been specifically designed for conversational AI applications, such as chatbots and virtual assistants. Recent advancements include more sophisticated dialogue management systems, emotional intelligence, and contextual understanding.\\n8. **Self-Supervised Learning**: Self-supervised learning techniques, like masked language modeling and next sentence prediction, allow models to learn from unlabeled data without explicit supervision. This approach enables the training of more generalizable and robust LLMs on a wide range of tasks and domains.\\n\\nSome notable recent models that embody these advancements include:\\n\\n* **BERT-MLM**: The BERT-MLM model improved upon the original BERT by incorporating masked language modeling to further enhance its performance.\\n* **Longformer**: The Longformer model uses a novel attention mechanism called \"token-level attention\" to enable faster and more efficient processing of long sequences.\\n* **XLNet**: XLNet incorporates self-supervised learning techniques, such as next sentence prediction, to improve upon the original BERT-MLM architecture.\\n\\nThese advances demonstrate the rapid progress being made in large language model research, with a focus on improving performance, efficiency, and versatility.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")"
      ],
      "metadata": {
        "id": "1ID4TY5flXwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "0ffd516d-1c23-49dc-bfbc-0dc535bd0768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Large language models (LLMs) have made significant progress in recent years, with new advancements in various areas such as performance, efficiency, and applications.\\n\\n1. **Multi-Task Learning**: Recent studies have shown that LLMs can be trained for multiple tasks simultaneously, which improves their overall performance and adaptability. For example, the BERT (Bidirectional Encoder Representations from Transformers) model was initially developed for natural language inference (NLI), but its multi-task learning capabilities led to significant improvements in other NLP tasks such as question answering and sentiment analysis [1].\\n2. **Efficient Transformer Architectures**: Researchers have made efforts to optimize transformer-based architectures, leading to faster and more efficient models. For instance, the T5 model, which uses a variant of the transformer architecture, achieved state-of-the-art results on several NLP benchmarks with 10-20 times fewer parameters than previous models [2].\\n3. **Self-Supervised Learning**: Self-supervised learning methods have become increasingly popular, allowing LLMs to learn from large amounts of unlabeled data without explicit supervision. This approach has been shown to be effective in training state-of-the-art models on several NLP tasks, including language modeling and text classification [3].\\n4. **Concurrent Training Methods**: Concurrent training methods have been proposed to train LLMs on multiple tasks simultaneously, which can lead to improved performance and adaptability. For example, the \"train-on-all-tasks\" approach has been shown to be effective in training state-of-the-art models for NLP tasks [4].\\n5. **Few-Shot Learning**: Few-shot learning methods have been developed to train LLMs on small amounts of labeled data, which can lead to improved performance and efficiency. For example, the \"few-shot\" approach has been shown to be effective in training state-of-the-art models for NLP tasks such as question answering and text classification [5].\\n6. **Explainability and Interpretability**: Researchers have made efforts to develop more explainable and interpretable LLMs, which can provide insights into their decision-making processes. For example, the \"attention-is-all-you-need\" approach has been shown to be effective in explaining the behavior of transformer-based architectures [6].\\n\\nThese advances demonstrate the ongoing progress and innovation in the field of large language models.\\n\\nReferences:\\n\\n[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2020). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 587-601).\\n\\n[2] T5: A Constrained-Layer Neural Model for Machine Translation (2020). arXiv preprint arXiv:1909.06149.\\n\\n[3] Guo, J., Weiss, R., & Shen, X. (2018). Meta-learning for few-shot learning in natural language processing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Volume 1 (Long Papers) (pp. 155-166).\\n\\n[4] Wang, Y., Lin, J., He, D., & Liu, M. (2020). Training multi-task models from scratch with a few samples per task. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Volume 1 (Long Papers) (pp. 141-151).\\n\\n[5] Xiong, Y., Wang, W., Zhang, J., & Lin, S. (2020). Few-shot learning with a novel neural architecture. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Volume 1 (Long Papers) (pp. 151-162).\\n\\n[6] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).\\n\\nNote: The references provided are a selection of the most relevant papers published in 2020 or later, and are intended to provide a snapshot of the current state of research in large language models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot Prompting**"
      ],
      "metadata": {
        "id": "A_4vNQMKll6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
        "# Returns positive sentiment"
      ],
      "metadata": {
        "id": "Kdut2ZvBliSi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7dc98208-25b0-4e5a-c360-00b107440fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The sentiment of the text is overwhelmingly positive.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
        "# Returns negative sentiment"
      ],
      "metadata": {
        "id": "IyDDZh_Ml0vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few-shot Promtping**\n",
        "Now we will try few-shot prompting on sentiment analysis.\n",
        "\n",
        "ğŸ‘‹ âš’ Provide your own examples for the few-shot prompting on this task. Prompt the model to provide a percentage of positive/neutral/negative for each example. Store the returned result in a variable 'out' and run through the results."
      ],
      "metadata": {
        "id": "GxpVlIwlmBb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke('''Please give me a percentage per sentiment (positive, neutral, negative) like this:\n",
        "  The product is ok, but did not meet my expectations. Positive: 30%, Neutral: 50%, Negative: 20%\n",
        "  The presentation was engaging and informative, but they spoke for over an hour. Positive: 80%, Neutral: 10%, Negative: 10%\n",
        "  This exam was extremely hard, but I managed. Positive: 30%, Neutral: 30%, Negative: 30%\n",
        "\n",
        "  It is late, but I will still make it to class on time. ?\n",
        "  This book was an emotional rollercoaster.\n",
        "''')\n",
        "\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "eD-AsDWvmADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role Prompting**\n",
        "Role or persona prompting assigns a role or persona to the model in the prompt with the assumption that the results will be more accurate. We can compare the first prompt without role to the second prompt with a role.\n",
        "\n",
        "The idea is that role prompting results in more technical benefits and drawbacks."
      ],
      "metadata": {
        "id": "CS_WKQ60tBW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out=llm.invoke(\"Explain the pros and cons of using PyTorch.\")\n",
        "\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "V2q1Tl9ooUMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
        "\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "4EpmYBmkoddq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain-of-Thought**"
      ],
      "metadata": {
        "id": "pcF_GeF3oqBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke(\"Who lived longer, Mozart or Elvis?\")\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "iGJ5wm1Gonds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke(\"Who lived longer, Mozart or Elvis? Let's think through this carefully, step by step.\")\n",
        "answer = out.split(\"\\n\\n\")\n",
        "for i in answer:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rfMkemhVo8Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Consistency**"
      ],
      "metadata": {
        "id": "0iWqX9fxpOeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " out = llm.invoke(\"John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Report the answer surrounded by backticks (example: 123)\")\n",
        " for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "1t32QrsDpM4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on a Dataset**\n",
        "\n",
        "In this part we will evaluate the performance of LLaMa on a questions-answering dataset that focuses on arithmetic tasks called [MultiArith](https://huggingface.co/datasets/ChilleD/MultiArith)."
      ],
      "metadata": {
        "id": "EZQbLc0zwwJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.16.1"
      ],
      "metadata": {
        "id": "JTbtijPlqayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyIMQ7IHFbIx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = \"ChilleD/MultiArith\"\n",
        "\n",
        "\n",
        "train_dataset = pd.DataFrame(load_dataset(dataset, split=\"train\"))[[\"question\", \"final_ans\"]]\n",
        "test_dataset = pd.DataFrame(load_dataset(dataset, split=\"test\"))[[\"question\", \"final_ans\"]]\n",
        "\n",
        "test_dataset.columns = [\"Question\", \"Gold Answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘‹ âš’ **Arithematic Question Answering**\n",
        "\n",
        "1. Write a prompt for answering the questions in the dataset.\n",
        "2. Post process the answers if needed.\n",
        "3. Compute the accuracy of the answers given by the LLMs.\n",
        "4. Find the examples where LLMs answer differently or wrongly."
      ],
      "metadata": {
        "id": "u6wiu2IpCJ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "GWLHdzaICuyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}